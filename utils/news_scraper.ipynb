{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29014e2b",
   "metadata": {},
   "source": [
    "### Generate Archive URLs based on pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0c53eb87-7d70-4707-8a61-2ed2666ba271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import datetime\n",
    "import pandas as pd \n",
    "\n",
    "# Set the base URL\n",
    "base_url = 'https://economictimes.indiatimes.com/archive/year-{},month-{}.cms'\n",
    "\n",
    "# Get the current year and month\n",
    "current_year = datetime.date.today().year\n",
    "current_month = datetime.date.today().month\n",
    "\n",
    "# Create a list of year and month\n",
    "year_month_list = []\n",
    "for year in range(2001, current_year+1):\n",
    "    for month in range(1, 13):\n",
    "        if year == current_year and month > current_month:\n",
    "            break\n",
    "        year_month_list.append(base_url.format(year, month))\n",
    "\n",
    "df = pd.DataFrame(year_month_list) \n",
    "df.to_csv(\"../data/urls_to_scrape.csv\", header=False,index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f6c8171",
   "metadata": {},
   "source": [
    "### Gather Archive links of each date using the previously generated links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bb14dda4-26e1-4cb6-a454-df56e4be355a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishn\\AppData\\Local\\Temp\\ipykernel_19136\\2894986945.py:13: DeprecationWarning: headless property is deprecated, instead use add_argument('--headless') or add_argument('--headless=new')\n",
      "  options.headless = True # it's more scalable to work in headless mode\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    " \n",
    "from selenium import webdriver \n",
    "from selenium.webdriver import Chrome \n",
    "from selenium.webdriver.chrome.service import Service \n",
    "from selenium.webdriver.common.by import By \n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# start by defining the options \n",
    "options = webdriver.ChromeOptions() \n",
    "options.headless = True # it's more scalable to work in headless mode \n",
    "# normally, selenium waits for all resources to download \n",
    "# we don't need it as the page also populated with the running javascript code. \n",
    "options.page_load_strategy = 'none' \n",
    "# this returns the path web driver downloaded \n",
    "chrome_path = ChromeDriverManager().install() \n",
    "chrome_service = Service(chrome_path) \n",
    "# pass the defined options and service objects to initialize the web driver \n",
    "driver = Chrome(options=options, service=chrome_service) \n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "daily_link = []\n",
    "\n",
    "# Scrape news articles from a website\n",
    "def scrape_news(url):\n",
    "    driver.get(url)\n",
    "    content=driver.find_element(By.ID, \"calender\").find_elements(By.TAG_NAME,\"a\")\n",
    "    for a in content:\n",
    "        url=\"https://economictimes.indiatimes.com\"+a.get_dom_attribute(\"href\")\n",
    "        daily_link.append(url)\n",
    "\n",
    "df=pd.read_csv(\"../data/urls_to_scrape.csv\",header=None)\n",
    "for url in df.values:\n",
    "    scrape_news(url[0])\n",
    "\n",
    "\n",
    "dfs = pd.DataFrame(daily_link) \n",
    "dfs.to_csv(\"../data/daily_links.csv\", header=False,index=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b76842f0",
   "metadata": {},
   "source": [
    "### Get individual news links for detailed content gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ac6de-79d4-4c2a-8694-82518d956b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver import Chrome \n",
    "from selenium.webdriver.chrome.service import Service \n",
    "from selenium.webdriver.common.by import By \n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "options = webdriver.ChromeOptions() \n",
    "options.headless = True\n",
    "options.page_load_strategy = 'none'\n",
    "chrome_path = ChromeDriverManager().install() \n",
    "chrome_service = Service(chrome_path) \n",
    "driver = Chrome(options=options, service=chrome_service) \n",
    "driver.implicitly_wait(1)\n",
    "# Print iterations progress\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()\n",
    "\n",
    "\n",
    "individual_news_links_list = []\n",
    "\n",
    "# Scrape news articles from a website\n",
    "def scrape_news(url):\n",
    "    driver.get(url)\n",
    "    \n",
    "    content=driver.find_elements(By.CLASS_NAME, \"contentbox5\")\n",
    "    if(content):\n",
    "        content.pop(0)\n",
    "        for table in content:\n",
    "            individual_news=table.find_elements(By.TAG_NAME,\"li\")\n",
    "            \n",
    "            if(individual_news):\n",
    "                # print(\"news count \"+str(individual_news.count))\n",
    "                for each_news_li in individual_news:\n",
    "                    individual_news_url=each_news_li.find_element(By.TAG_NAME,\"a\").get_attribute(\"href\")\n",
    "                    # print(individual_news_url)\n",
    "                    individual_news_links_list.append(individual_news_url)\n",
    "                    # print(url)\n",
    "\n",
    "df=pd.read_csv(\"../data/daily_links.csv\",header=None)\n",
    "length=df[0].count()\n",
    "progress=0\n",
    "printProgressBar(progress, length, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "for url in df.values:\n",
    "    progress=progress+1\n",
    "    printProgressBar(progress, length, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "    # print(url[0])\n",
    "    scrape_news(url[0])\n",
    "\n",
    "\n",
    "dfs = pd.DataFrame(individual_news_links_list) \n",
    "dfs.to_csv(\"../data/individual_news_links.csv\", header=False,index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9733117a",
   "metadata": {},
   "source": [
    "### Individual News Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93f6ab3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishn\\AppData\\Local\\Temp\\ipykernel_3284\\4276475783.py:10: DeprecationWarning: headless property is deprecated, instead use add_argument('--headless') or add_argument('--headless=new')\n",
      "  options.headless = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The developed world, especially the United States, may not like this but in 2008 Japan would be sending into space the world’s first Greenhouse Gases Observing Satellite (GOSAT) which would not only specify the quantum of emission by every region but also observe and analyse the concentration of greenhouse gases in all continents.\n",
      "The GOSAT data will make it possible to ascertain the global distribution of carbon dioxide and methane and the geographical distribution of all seasonal and inter-annual variation in the flux of greenhouse gases.\n",
      "The analysis will not only contribute to a deeper scientific understanding of the behaviour of causative global warming agents, but also provide fundamental information to refine climate change predictions and, thus, formulate counter measures. The study will also help in environmental administration efforts such as ascertaining the amount of CO2 absorbed or released in every region as also evaluate the carbon balance in global forests.\n",
      "The project is a joint effort by Japan’s Ministry of Environment, the National Institute for Environmental Studies (NIES) and the Japan Aerospace Exploration Agency.\n",
      "Over the last century, expansion of industrial activity has resulted in the rise of carbon dioxide, a greenhouse gas which leads to an increase in atmospheric temperature. Methane, nitrous oxide and halocarbons are also greenhouse gases but Co2 and methane account for 81 per cent of the total greenhouse effect caused the world over.\n",
      "In order for every nation on the globe to adopt measures for reducing greenhouse gas emissions, it is essential to set rational goals based on accurate predictions of climate change and its impact. It is also important that emission levels per country are specifically determined so that reduction measures can accordingly be strategised.\n",
      "NEW YORK: The International Year of Sanitation, 2008, is a theme year set by the UN General Assembly to help put this global crisis at the forefront of the international agenda. Every year inadequate water, sanitation and hygiene contribute to the deaths of 1.5 million children.\n",
      "“An estimated 42,000 people die every week from diseases related to low water quality and an absence of adequate sanitation. This situation is unacceptable,” Secretary-General Ban Ki-moon said. “Access to sanitation is deeply connected to virtually all the Millennium Development Goals, in particular those involving the environment, education, gender equality and the reduction of child mortality and poverty,” Secretary-General Ban Ki-moon said.\n",
      "\"Today, we go from a stage of planning to one of implementation,” said His Royal Highness Prince Willem-Alexander of the Netherlands, Chairperson of the United Nations Secretary-General’s Advisory Board on Water and Sanitation (UNSGAB).\n",
      "Though more than 1.2 billion people worldwide have gained access to improved sanitation between 1990 and 2004, an estimated 2.6 billion people - including 980 million children - have lagged behind. The world needs to accelerate progress in order to meet the Millennium Development Goal target to reduce by half the proportion of people without access to basic sanitation by 2015.\n",
      "If current trends continue, there will be 2.4 billion people without basic sanitation in 2015, with children continuing to pay the price in lost lives, missed schooling, in disease, malnutrition and poverty.\n",
      "The launch of the theme year, which runs through 2008, was organized by the UN Department of Economic and Social Affairs (UNDESA) in collaboration with the UN-Water Task Force on Sanitation. The event was attended by UN Member States, NGOs, citizen groups, academics and the private sector as well as members of the Secretary-General’s Advisory Board.\n",
      "It is estimated that improved sanitation facilities could reduce diarrhea-related deaths in young children by more than one-third. If hygiene promotion is added, such as teaching proper hand washing, deaths could be reduced by two thirds. It would also help accelerate economic and social development in countries where poor sanitation is a major cause of lost work and school days because of illness.\n",
      "Progress requires broad cooperation through public and private partnerships, community involvement and public awareness. Investing approximately $10 billion per year can halve the proportion of people without basic sanitation by 2015. If sustained, the same investment could achieve basic sanitation for the entire world within one or two decades. This sum is less than one per cent of world military spending in 2005, one-third of the estimated global spending on bottled water, or about as much as Europeans spend on ice cream each year. While the funding needed for sanitation is not overwhelmingly large, the return on that investment is potentially great.\n",
      "Millions more people across the world are going to be at risk from flooding in the future because of climate change and population increase. So if you thought climate change would affect only rural areas of the world, you can think again.\n",
      "By 2070, Kolkata would tumble Mumbai in becoming one of the nine most risky cities in Asia in context to coastal flooding. Currently Mumbai has the highest number of people exposed to coastal flooding but by 2070 Kolkata will be the most vulnerable, with the exposed population expected to increase over seven times to more than 14 million people.\n",
      "The impact of climate change and urban development is set to more than triple the number of people exposed to coastal flooding by 2070, according to a detailed study done by the Organisation for Economic and Co-operation Development (OECD). It says that city-scale disasters will become regular events across the world.\n",
      "The study analysed 130 key port cities worldwide to investigate the impact of climate change alongside subsidence, population growth and urban economic development. It focused on the exposure of people and property and infrastructure to a 1-in-100 year flood event now and in the future. London was ranked 41st in a list for population exposed in 2070 and 27th for property and infrastructure assets exposed.\n",
      "By 2070, eight of the most exposed cities will be in Asia. As of now, Guangzhou is the second most exposed city in terms of assets, followed by New York, Kolkata, Shanghai, Mumbai, Tianjin, Tokyo, Hong Kong, and Bangkok, respectively.\n",
      "Over the coming decades, the unprecedented growth and development of Asian mega-cities will be a key factor in driving the increase in coastal flood risk globally. In terms of population exposure, Kolkata will be closely followed by Mumbai, Dhaka, Guangzhou, Ho Chi Minh City, Shanghai, Bangkok and Rangoon.\n",
      "Miami is in the ninth place and would be the only top 10 city situated in a developed country, while Hai Phong (Vietnam) is ranked tenth. Miami is the most exposed city today and will remain so in 2070, with exposed assets rising from approximately US$400 billion today to over US$3.5 trillion.\n",
      "The cities with the highest value of property and infrastructure assets exposed to coastal flooding today are primarily in developed countries.\n",
      "KOLKATA: The Union Government has sanctioned Rs 220 crore for the second phase of Ganga action plan work in West Bengal.\n",
      "“The Centre had sanctioned Rs 180 crore for the first phase work along the Ganga embankment areas in the state during the period of 1985-2000, ”says the Union minister of state for Environment N N Meena.\n",
      "West Bengal minister for Municipal Affairs and Urban Development Ashok Bhattacharya said of the Rs 220 crore allotted for the ongoing second phase work, the state government has received an amount of Rs 170 crore of which 70 per cent has already been spent. The work is aimed at strengthening the embankment area.\n",
      "About river front beautification plans and improvement of eight ghats along the river bank in city area, the Union Minister said 70 per cent of the Rs 4.81 crore project for improvement of ghats will be borne by the Centre and the rest by the state government.\n",
      "He said the Centre sanctioned Rs 135 crore for a common effluent treatment plant in Bantala area, near the city.\n",
      "To a question, the Minister said the Centre has decided to preserve rivers in 20 states.\n",
      "\"An estimated Rs 1,000 crore will be spent for the purpose,\" he said.\n",
      "Meena was speaking to media after laying foundation stone of the renovation and beautification project of eight ghats along the river.\n",
      "NEW DELHI: The Delhi government should roll back the additional cess of 25 paise per litre on diesel, as the decision will put the common man under ‘huge burden’, Industry body Assocham today said.\n",
      "\"The proposed cess would serve no intended purpose and make the government of Delhi highly unpopular as fuel prices are already very high and put common man under huge burden,\" Assocham Secretary General D S Rawat said in a statement.\n",
      "He said the government should phase out diesel-run vehicles in a time-bound manner to curb air pollution instead of imposing cess.\n",
      "The chamber said pollution level in Delhi has increased due to many other factors and not just due to diesel-run vehicles, and added that the proposed cess is not a viable solution to discourage use of such vehicles in and around the capital.\n",
      "The Delhi government should further intensify its efforts to ensure that state-run oil companies and private refiners produce sulphur-free diesel, which is less polluting compared to what is being sold in the market, it said.\n",
      "\"In the last six to seven years, Ministry of Petroleum and Natural Gas has been consistently asking state-owned oil companies and private sector refining firms to eliminate sulphur element in diesel. If further pressure is built on them, the oil companies have the potential to produce sulphur free diesel so that it does not add to air pollution\", Assocham added.\n",
      "It suggested that the Delhi government and the Union Petroleum and Natural Gas Ministry should allot budget to the oil companies for introducing sulphur-free diesel with technological upgradation in refineries.\n",
      "If world leaders do not immediately engage in a race against time to save the Earth's coral reefs, these vital ecosystems will not survive the global warming and acidification predicted for later this century. That is the conclusion of a group of marine scientists from around the world in a major new study published in the journal Science.\n",
      "It's vital that the public understands that the lack of sustainability in the world's carbon emissions is causing the rapid loss of coral reefs, the world's most biodiverse marine ecosystem,\" said Drew Harvell, Cornell professor of ecology and evolutionary biology and head of the Coral Disease Research Team, which is part of the international Coral Reef Targeted Research (CRTR) group that wrote the new study.\n",
      "The rise of carbon dioxide emissions and the resultant climate warming from the burning of fossil fuels are making oceans warmer and more acidic, said co-author Harvell, which is triggering widespread coral disease and stifling coral growth toward \"a tipping point for functional collapse.\"\n",
      "The 17 marine scientists who authored the new study argue that \"drastic action\" is needed from world leaders to turn around the trend in rising levels of atmospheric carbon dioxide (CO2) to protect coral reefs. They based their conclusions on the forecasts for rising global temperatures and levels of CO2 announced recently by the Intergovernmental Panel on Climate Change, a United Nations body.\n",
      "\"Coral reefs have already taken a big hit from recent warm temperatures, but rapid rises in carbon dioxide cause acidification, which adds a new threat: the inability of corals to create calcareous skeletons,\" said Harvell. \"Acidification actually threatens all marine animals and plants with calcareous skeletons, including corals, snails, clams and crabs. Our study shows that levels of CO2 could become unsustainable for coral reefs in as little as five decades.\"\n",
      "In the short term, better management of overfishing and local stressors may increase resilience of reefs to climate threats, but rising global CO2 emissions will rapidly outstrip the capacity of local coastal managers and policy-makers to maintain the health of these critical ecosystems if the emissions continue unchecked, the authors stress.\n",
      "At stake, added Ove Hoegh-Guldberg, director of the Center for Marine Studies at the University of Queensland, Australia, and the study's senior author, are ecosystems that play vital roles in providing habitats for a vast array of marine species that are essential to the oceans' complex food chain. They also provide livelihoods to 100 million people who live along the coasts of tropical developing countries. Diving tourism in the Caribbean alone is estimated to generate more than $100 billion a year. The loss of coral reef ecosystems also is exposing people to flooding, coastal erosion and the loss of food and income from reef-based fisheries and tourism, he added.\n",
      "Thanks to Google earth. It is now easy to find out exactly how air pollution is affecting your neighborhoods and neighbourhoods around the world. Google Earth has always been a very cool way to see the world, and recently they have added some very green features including green buildings, and now the ability to visualize air pollution!\n",
      "With the new Google Earth features, you will be able to find out the sources of pollution from the major point sources in the United States. These include cement facilities, manufacturing plants, refineries and electric generating units. Each of these will provide information on the carbon monoxide, lead, nitrogen oxide, VOCs, particulate matter and sulfur dioxide. The data is displayed in graph form, and you can also visually compare each emitter to the next, thanks to the location of the height of each marker.\n",
      "The information from the EPA can be specified to be from a specific state, from an emission sector, and pollutant amount. Now if we could get this for the entire planet.\n",
      "MOSCOW: A Hummer owner in Russia's second city St. Petersburg has given antiglobalists the green light to pelt his oversized vehicle with rotten eggs, Russian news agencies reported on Wednesday.\n",
      "\"Peter Antiglobalist\" activists told news agency RIA they found a driver willing to let them express their dissatisfaction with consumerism by throwing things at his luxury sport utility vehicle, a spokesman said.\n",
      "A Moscow car dealer puts the base price of a Hummer H3 at $49,500.\n",
      "\"Luxury is a false value, clouding modern society's vision. Advertising posters, TV shows and slick marketing constantly tells us that buying things is the most important value in our society,\" RIA quoted the spokesman as saying.\n",
      "The antiglobalists said throwing eggs and tomatoes at the Hummer will help draw attention to their cause.\n",
      "The vehicle's owner said he will then sell it and donate the proceeds to an orphanage.\n",
      "SRINAGAR: Stressing on the need to conserve the world famous Dal lake, Jammu and Kashmir chief secretary B R Kundal underscored the need of a collective and coordinated approach to restore its pristine glory.\n",
      "The famous fresh water lake demands attention from one and all for protection and conservation, Kundal said while addressing a high level meeting of Lakes and Waterways Development Authority (LAWDA) and other departments concerned with protection of water bodies and environment, an official spokesman said.\n",
      "He said the meeting discussed threadbare the issues pertaining to Dal development, removal of encroachments and related matters.\n",
      "Meanwhile, state Forest Minister Qazi Mohammad Afzal today said that about 10 projects worth over Rs 21 crores are being implemented to ensure better road connectivity in the Ganderbal district of the state, which he represents.\n",
      "Three road projects worth Rs 3.68 crore are under execution under Pradhan Mantri Gram Sadak Yojana (PMGSY) and would be completed by the end of May 2008, while five other road projects costing Rs 8.83 crore would be taken up next month under Bharat Nirman programme, Qazi said during a visit to his home constituency.\n",
      "He said the upgradation of two more road projects including Lar-Barsoo-Manigam and Nagbal-Bakuroo at a cost of Rs 9 crore would also be taken up during the coming month.\n",
      "To improve power distribution in the area, work on power receiving station at Tulabagh Sheer Pathri would be taken up during next month, Qazi said adding that works on health centres at Kachan and Kurhama was also in progress.\n",
      "AIZAWL: Hordes of rats spawned by gregarious bamboo flowering ravaged crops in Mizoram and what remained was mostly destroyed by unprecedented heavy rains, leaving the state battling famine a year before the Assembly elections.\n",
      "The ruling Mizo National Front, which came into being after \"Mautam\" (death of bamboo) famine in the 1950s under Laldenga as Mizo National Famine Front, declared the state a \"disaster area\", sought an additional Rs 187 crore from the Centre but found itself unable to deal with the crisis.\n",
      "\"Mautam\" is an ecological phenomenon occurring in some northeast states in a 48-year cycle and scientists say rats multiply by the millions after consuming seeds of the bamboo flower (melocanna baccifera) before turning to other crops.\n",
      "The year 2007 saw over 95 per cent bamboo plants in the agrarian state flower and rats destroy tobacco, cucumbers, pumpkins, grapes and other fruits and vegetables, while paddy cultivation came down by 75 per cent and maize to practically zero due to farmers' apprehensions of an impending famine.\n",
      "Reduction of the foodgrains quota from the Centre and heavy monsoon rains that triggered landslides, leaving homes and fish ponds destroyed further compounded the problem.\n",
      "Finding the Rs 20 crore allocated under Bamboo Flowering and Famine Combat Scheme too meagre, the Zoramthanga government doubled wages for farmers under the scheme and hiked the rice quota from 2 to 3 kg per adult per week.\n",
      "On the political front, the year saw formation of Mizoram Democratic Movement which declared it would field independents in the 2008 assembly polls and Mipui Pawl (People's Front) floated by former state chief secretary M Lalmanzuala.\n",
      "Reducing infant mortality and chronic malnutrition, mitigating the impact of natural disasters, and slashing the toll of domestic violence, sexual exploitation and abuse are among the immediate challenges facing the United Nations Children’s Fund (UNICEF) in Latin America and the Caribbean in 2008.\n",
      "\"On child survival, we must continue to focus on the critical period after a mother gives birth and an infant’s good start in life,\" UNICEF Regional Director Nils Kastberg said in a statement today, underlining the need for more public investment.\n",
      "He called for greater support to increase breastfeeding and better prevent mother-to-child transmission of HIV, while also providing HIV-positive mothers with treatment that would allow them to live to see their children grow up.\n",
      "Preparedness before natural disasters must be improved to cut down on the catastrophic impact they have on families and towns and emergency responses must be accelerated to avoid loss of lives and livelihoods, which tend to affect women and children first, he added.\n",
      "In a region where 80,000 young people die every year as a result of violence in the home, 2 million suffer commercial sexual exploitation and 6 million suffer severe abuse each year, remedial measures are crucial and urgent, Mr. Kastberg stressed.\n",
      "He also called for more funds dedicated to programmes to create opportunities for adolescent development. \"Specifically, we need to ensure that the 25 per cent to 30 per cent of adolescents and young people between 15 and 24 years of age, who are out of school or out of work, be better prepared to formally enter the working world,\" he said.\n",
      "An important element to achieving this would be to expand basic education beyond primary school to include education from pre-primary to secondary, and to make it intercultural, of good quality and open to the different languages in the national cultural context.\n",
      "\"With 2008 being the international year of languages, it is timely to focus on that element of education,\" Mr. Kastberg said. \"By providing a full and proper education, we can build a full and proper work force of young people.\"\n",
      "In coordination with the UN Economic Commission for Latin America and the Caribbean (ECLAC) and other UN agencies, one of UNICEF’s challenges in 2008 will be to develop a reliable system to gather pertinent information from sub-national level, which would better reflect the social realities and disparities of the region.\n",
      "Summing up UNICEF’s programme for 2008, Mr. Kastberg concluded: \"We would hope the end result of such efforts would mean that by this time next year, we would see an important shift for and among young people; that they would feel more confident of their role, their place and their rights in building the region – confident that change was happening with them and for them, and not at their expense.\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39m# print(download[0])\u001b[39;00m\n\u001b[0;32m     49\u001b[0m scrape_news(download[\u001b[39m0\u001b[39m])\n\u001b[1;32m---> 50\u001b[0m scrape_news(download[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m     51\u001b[0m \u001b[39m# scrape_news(download[2])\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[39m# scrape_news(download[3])\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m# scrape_news(download[4])\u001b[39;00m\n\u001b[0;32m     54\u001b[0m i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n",
      "Cell \u001b[1;32mIn[8], line 25\u001b[0m, in \u001b[0;36mscrape_news\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mscrape_news\u001b[39m(url):\n\u001b[0;32m     23\u001b[0m     driver\u001b[39m.\u001b[39mget(url)\n\u001b[1;32m---> 25\u001b[0m     content\u001b[39m=\u001b[39mdriver\u001b[39m.\u001b[39;49mfind_element(By\u001b[39m.\u001b[39;49mTAG_NAME, \u001b[39m\"\u001b[39;49m\u001b[39marticle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     26\u001b[0m     \u001b[39mif\u001b[39;00m(content):\n\u001b[0;32m     27\u001b[0m         text\u001b[39m=\u001b[39mcontent\u001b[39m.\u001b[39mfind_element(By\u001b[39m.\u001b[39mCLASS_NAME,\u001b[39m\"\u001b[39m\u001b[39mmedium\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mtext\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:831\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    828\u001b[0m     by \u001b[39m=\u001b[39m By\u001b[39m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    829\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[name=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 831\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mFIND_ELEMENT, {\u001b[39m\"\u001b[39;49m\u001b[39musing\u001b[39;49m\u001b[39m\"\u001b[39;49m: by, \u001b[39m\"\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m\"\u001b[39;49m: value})[\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:438\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    435\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m params:\n\u001b[0;32m    436\u001b[0m         params[\u001b[39m\"\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession_id\n\u001b[1;32m--> 438\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcommand_executor\u001b[39m.\u001b[39;49mexecute(driver_command, params)\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[0;32m    440\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_handler\u001b[39m.\u001b[39mcheck_response(response)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:290\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    288\u001b[0m data \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mdump_json(params)\n\u001b[0;32m    289\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_url\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 290\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(command_info[\u001b[39m0\u001b[39;49m], url, body\u001b[39m=\u001b[39;49mdata)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:311\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    308\u001b[0m     body \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 311\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conn\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[0;32m    312\u001b[0m     statuscode \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus\n\u001b[0;32m    313\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\urllib3\\_request_methods.py:118\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_encode_url(\n\u001b[0;32m    111\u001b[0m         method,\n\u001b[0;32m    112\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39murlopen_kw,\n\u001b[0;32m    116\u001b[0m     )\n\u001b[0;32m    117\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_encode_body(\n\u001b[0;32m    119\u001b[0m         method, url, fields\u001b[39m=\u001b[39;49mfields, headers\u001b[39m=\u001b[39;49mheaders, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49murlopen_kw\n\u001b[0;32m    120\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\urllib3\\_request_methods.py:217\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    213\u001b[0m     extra_kw[\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mContent-Type\u001b[39m\u001b[39m\"\u001b[39m, content_type)\n\u001b[0;32m    215\u001b[0m extra_kw\u001b[39m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 217\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\urllib3\\poolmanager.py:433\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    431\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 433\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(method, u\u001b[39m.\u001b[39;49mrequest_uri, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    435\u001b[0m redirect_location \u001b[39m=\u001b[39m redirect \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mget_redirect_location()\n\u001b[0;32m    436\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    791\u001b[0m     conn,\n\u001b[0;32m    792\u001b[0m     method,\n\u001b[0;32m    793\u001b[0m     url,\n\u001b[0;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[0;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[0;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[0;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[0;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[0;32m    803\u001b[0m )\n\u001b[0;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[0;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\urllib3\\connection.py:454\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    453\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 454\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    456\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    457\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1373\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1375\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver import Chrome \n",
    "from selenium.webdriver.chrome.service import Service \n",
    "from selenium.webdriver.common.by import By \n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "options = webdriver.ChromeOptions() \n",
    "options.headless = True\n",
    "options.page_load_strategy = 'none'\n",
    "chrome_path = ChromeDriverManager().install() \n",
    "chrome_service = Service(chrome_path)\n",
    "driver = Chrome(options=options, service=chrome_service)\n",
    "driver.implicitly_wait(2)\n",
    "\n",
    "daily_link = []\n",
    "\n",
    "def multiple_drivers(count):\n",
    "    driver_list=[]\n",
    "    for _ in range(count):\n",
    "        driver = Chrome(options=options, service=chrome_service)\n",
    "        driver.implicitly_wait(2)\n",
    "        driver_list.insert(driver)\n",
    "    return driver_list\n",
    "\n",
    "def replace_multiline(text):\n",
    "    return text.replace(\"\\n\\n\",\"\\n\")\n",
    "# Scrape news articles from a website\n",
    "def scrape_news(url):\n",
    "    driver.get(url)\n",
    "    \n",
    "    content=driver.find_element(By.TAG_NAME, \"article\")\n",
    "    if(content):\n",
    "        text=content.find_element(By.CLASS_NAME,\"medium\").text\n",
    "        text=replace_multiline(text)\n",
    "        print(text)\n",
    "\n",
    "df=pd.read_csv(\"../data/individual_news_links_100.csv\",header=None)\n",
    "\n",
    "def parallel_data(data_from,count):\n",
    "    return_array = []\n",
    "    i = data_from\n",
    "    while i < data_from+count:\n",
    "        if(df.values[i]):\n",
    "            return_array.append(df.values[i][0])\n",
    "            # print(df.values[i][0])\n",
    "        i += 1\n",
    "    return return_array\n",
    "\n",
    "i = 0\n",
    "c=df[0].count()\n",
    "count=5\n",
    "multiple_drivers(count)\n",
    "while i < c:\n",
    "    #parallel download\n",
    "    download=parallel_data(i,count)\n",
    "    for _ in multiple_drivers:\n",
    "        scrape_news(download[0])\n",
    "    # print(download[0])\n",
    "    scrape_news(download[1])\n",
    "    # scrape_news(download[2])\n",
    "    # scrape_news(download[3])\n",
    "    # scrape_news(download[4])\n",
    "    i += 5\n",
    "\n",
    "# for url in df.values:\n",
    "#     print(url[0])\n",
    "#     scrape_news(url[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63d19f84",
   "metadata": {},
   "source": [
    "### Sentimental analysis Model train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6e5e25bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['neutral']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Loading the Dataset\n",
    "import pandas as pd\n",
    "data = pd.read_csv('../data/financial_sentimental_data.csv')\n",
    "#Pre-Prcoessing and Bag of Word Vectorization using Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(data['Sentence'])\n",
    "\n",
    "# print(text_counts.vocabulary_)\n",
    "# print(text_counts)\n",
    "#Splitting the data into trainig and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, data['Sentiment'], test_size=0.25, random_state=5)\n",
    "\n",
    "# Training the model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, Y_train)\n",
    "# #Caluclating the accuracy score of the model\n",
    "from sklearn import metrics\n",
    "# print(X_test[1])\n",
    "new_text = [\"The GeoSolutions technology will leverage Benefon 's GPS solutions by providing Location Based Search Technology , a Communities Platform\"]\n",
    "N_test=cv.transform(new_text)\n",
    "# print(N_test)\n",
    "print(\"\")\n",
    "# print(X_train[0])\n",
    "predicted = MNB.predict(N_test)\n",
    "# print(cv.vocabulary_)\n",
    "\n",
    "print(predicted)\n",
    "# accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "# print(\"Accuracuy Score: \",accuracy_score)\n",
    "# # text_counts1 = cv.fit_transform()\n",
    "# X_new=[[\"This stock is going down in the last 2 months rapidly\"],[\"This stock is going +0.03% every day for the last 2 weeks\"]]\n",
    "# ynew = MNB.predict(X_new)\n",
    "# # print(text_counts1)\n",
    "# # predicted1=MNB.predict(X_new[0])\n",
    "\n",
    "# print(ynew)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
