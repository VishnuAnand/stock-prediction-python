{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29014e2b",
   "metadata": {},
   "source": [
    "### Generate Archive URLs based on pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0c53eb87-7d70-4707-8a61-2ed2666ba271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import datetime\n",
    "import pandas as pd \n",
    "\n",
    "# Set the base URL\n",
    "base_url = 'https://economictimes.indiatimes.com/archive/year-{},month-{}.cms'\n",
    "\n",
    "# Get the current year and month\n",
    "current_year = datetime.date.today().year\n",
    "current_month = datetime.date.today().month\n",
    "\n",
    "# Create a list of year and month\n",
    "year_month_list = []\n",
    "for year in range(2001, current_year+1):\n",
    "    for month in range(1, 13):\n",
    "        if year == current_year and month > current_month:\n",
    "            break\n",
    "        year_month_list.append(base_url.format(year, month))\n",
    "\n",
    "df = pd.DataFrame(year_month_list) \n",
    "df.to_csv(\"../data/urls_to_scrape.csv\", header=False,index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f6c8171",
   "metadata": {},
   "source": [
    "### Gather Archive links of each date using the previously generated links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bb14dda4-26e1-4cb6-a454-df56e4be355a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vishn\\AppData\\Local\\Temp\\ipykernel_19136\\2894986945.py:13: DeprecationWarning: headless property is deprecated, instead use add_argument('--headless') or add_argument('--headless=new')\n",
      "  options.headless = True # it's more scalable to work in headless mode\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    " \n",
    "from selenium import webdriver \n",
    "from selenium.webdriver import Chrome \n",
    "from selenium.webdriver.chrome.service import Service \n",
    "from selenium.webdriver.common.by import By \n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# start by defining the options \n",
    "options = webdriver.ChromeOptions() \n",
    "options.headless = True # it's more scalable to work in headless mode \n",
    "# normally, selenium waits for all resources to download \n",
    "# we don't need it as the page also populated with the running javascript code. \n",
    "options.page_load_strategy = 'none' \n",
    "# this returns the path web driver downloaded \n",
    "chrome_path = ChromeDriverManager().install() \n",
    "chrome_service = Service(chrome_path) \n",
    "# pass the defined options and service objects to initialize the web driver \n",
    "driver = Chrome(options=options, service=chrome_service) \n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "daily_link = []\n",
    "\n",
    "# Scrape news articles from a website\n",
    "def scrape_news(url):\n",
    "    driver.get(url)\n",
    "    content=driver.find_element(By.ID, \"calender\").find_elements(By.TAG_NAME,\"a\")\n",
    "    for a in content:\n",
    "        url=\"https://economictimes.indiatimes.com\"+a.get_dom_attribute(\"href\")\n",
    "        daily_link.append(url)\n",
    "\n",
    "df=pd.read_csv(\"../data/urls_to_scrape.csv\",header=None)\n",
    "for url in df.values:\n",
    "    scrape_news(url[0])\n",
    "\n",
    "\n",
    "dfs = pd.DataFrame(daily_link) \n",
    "dfs.to_csv(\"../data/daily_links.csv\", header=False,index=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b76842f0",
   "metadata": {},
   "source": [
    "### Get individual news links for detailed content gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ac6de-79d4-4c2a-8694-82518d956b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver import Chrome \n",
    "from selenium.webdriver.chrome.service import Service \n",
    "from selenium.webdriver.common.by import By \n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "options = webdriver.ChromeOptions() \n",
    "options.headless = True\n",
    "options.page_load_strategy = 'none'\n",
    "chrome_path = ChromeDriverManager().install() \n",
    "chrome_service = Service(chrome_path) \n",
    "driver = Chrome(options=options, service=chrome_service) \n",
    "driver.implicitly_wait(1)\n",
    "# Print iterations progress\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = 'â–ˆ', printEnd = \"\\r\"):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()\n",
    "\n",
    "\n",
    "individual_news_links_list = []\n",
    "\n",
    "# Scrape news articles from a website\n",
    "def scrape_news(url):\n",
    "    driver.get(url)\n",
    "    \n",
    "    content=driver.find_elements(By.CLASS_NAME, \"contentbox5\")\n",
    "    if(content):\n",
    "        content.pop(0)\n",
    "        for table in content:\n",
    "            individual_news=table.find_elements(By.TAG_NAME,\"li\")\n",
    "            \n",
    "            if(individual_news):\n",
    "                # print(\"news count \"+str(individual_news.count))\n",
    "                for each_news_li in individual_news:\n",
    "                    individual_news_url=each_news_li.find_element(By.TAG_NAME,\"a\").get_attribute(\"href\")\n",
    "                    # print(individual_news_url)\n",
    "                    individual_news_links_list.append(individual_news_url)\n",
    "                    # print(url)\n",
    "\n",
    "df=pd.read_csv(\"../data/daily_links.csv\",header=None)\n",
    "length=df[0].count()\n",
    "progress=0\n",
    "printProgressBar(progress, length, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "for url in df.values:\n",
    "    progress=progress+1\n",
    "    printProgressBar(progress, length, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "    # print(url[0])\n",
    "    scrape_news(url[0])\n",
    "\n",
    "\n",
    "dfs = pd.DataFrame(individual_news_links_list) \n",
    "dfs.to_csv(\"../data/individual_news_links.csv\", header=False,index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9733117a",
   "metadata": {},
   "source": [
    "### Individual News Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f6ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver import Chrome \n",
    "from selenium.webdriver.chrome.service import Service \n",
    "from selenium.webdriver.common.by import By \n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "options = webdriver.ChromeOptions() \n",
    "options.headless = True\n",
    "options.page_load_strategy = 'none'\n",
    "chrome_path = ChromeDriverManager().install() \n",
    "chrome_service = Service(chrome_path) \n",
    "driver = Chrome(options=options, service=chrome_service) \n",
    "driver.implicitly_wait(2)\n",
    "\n",
    "daily_link = []\n",
    "\n",
    "def replace_multiline(text):\n",
    "    return text.replace(\"\\n\\n\",\"\\n\")\n",
    "# Scrape news articles from a website\n",
    "def scrape_news(url):\n",
    "    driver.get(url)\n",
    "    \n",
    "    content=driver.find_element(By.TAG_NAME, \"article\")\n",
    "    if(content):\n",
    "        text=content.find_element(By.CLASS_NAME,\"medium\").text\n",
    "        text=replace_multiline(text)\n",
    "        print(text)\n",
    "\n",
    "df=pd.read_csv(\"../data/individual_news_links_100.csv\",header=None)\n",
    "\n",
    "def parallel_data(data_from,count):\n",
    "    return_array = []\n",
    "    i = data_from\n",
    "    while i < data_from+count:\n",
    "        if(df.values[i]):\n",
    "            return_array.append(df.values[i])\n",
    "            print(df.values[i])\n",
    "        i += 1\n",
    "\n",
    "i = 0\n",
    "c=df[0].count()\n",
    "while i < c:\n",
    "    #parallel download\n",
    "    parallel_data(i,5)\n",
    "    i += 5\n",
    "\n",
    "# for url in df.values:\n",
    "#     print(url[0])\n",
    "#     scrape_news(url[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63d19f84",
   "metadata": {},
   "source": [
    "### Sentimental analysis Model train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6e5e25bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['neutral']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Loading the Dataset\n",
    "import pandas as pd\n",
    "data = pd.read_csv('../data/financial_sentimental_data.csv')\n",
    "#Pre-Prcoessing and Bag of Word Vectorization using Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
    "text_counts = cv.fit_transform(data['Sentence'])\n",
    "\n",
    "# print(text_counts.vocabulary_)\n",
    "# print(text_counts)\n",
    "#Splitting the data into trainig and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, data['Sentiment'], test_size=0.25, random_state=5)\n",
    "\n",
    "# Training the model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train, Y_train)\n",
    "# #Caluclating the accuracy score of the model\n",
    "from sklearn import metrics\n",
    "# print(X_test[1])\n",
    "new_text = [\"The GeoSolutions technology will leverage Benefon 's GPS solutions by providing Location Based Search Technology , a Communities Platform\"]\n",
    "N_test=cv.transform(new_text)\n",
    "# print(N_test)\n",
    "print(\"\")\n",
    "# print(X_train[0])\n",
    "predicted = MNB.predict(N_test)\n",
    "# print(cv.vocabulary_)\n",
    "\n",
    "print(predicted)\n",
    "# accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
    "# print(\"Accuracuy Score: \",accuracy_score)\n",
    "# # text_counts1 = cv.fit_transform()\n",
    "# X_new=[[\"This stock is going down in the last 2 months rapidly\"],[\"This stock is going +0.03% every day for the last 2 weeks\"]]\n",
    "# ynew = MNB.predict(X_new)\n",
    "# # print(text_counts1)\n",
    "# # predicted1=MNB.predict(X_new[0])\n",
    "\n",
    "# print(ynew)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
